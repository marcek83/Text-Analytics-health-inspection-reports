{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MA069ja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nlppreprocess import NLP\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'CQC_documents_df_V2.csv' does not exist: b'CQC_documents_df_V2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-355fe1478f0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CQC_documents_df_V2.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'CQC_documents_df_V2.csv' does not exist: b'CQC_documents_df_V2.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"CQC_documents_df_V2.csv\", index_col = 0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents count\n",
    "Key stats about the size and columns of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(927, 25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['doc_index_first_line', 'doc_details', 'doc_index_last_line',\n",
       "       'full_text', 'filename', 'providerId', 'locationId',\n",
       "       'organisationType', 'type', 'name', 'region', 'postalCode',\n",
       "       'onspdLatitude', 'onspdLongitude', 'rating_overall', 'reportDate',\n",
       "       'rating_caring', 'rating_effective', 'rating_responsive',\n",
       "       'rating_safe', 'rating_wellled', 'URL', 'Location_type',\n",
       "       'Location_subtype', 'Report_URL'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index_first_line</th>\n",
       "      <th>doc_index_last_line</th>\n",
       "      <th>onspdLatitude</th>\n",
       "      <th>onspdLongitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>9.270000e+02</td>\n",
       "      <td>9.270000e+02</td>\n",
       "      <td>927.000000</td>\n",
       "      <td>927.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>1.431062e+06</td>\n",
       "      <td>1.433914e+06</td>\n",
       "      <td>52.338929</td>\n",
       "      <td>-1.213135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>7.536094e+05</td>\n",
       "      <td>7.531904e+05</td>\n",
       "      <td>1.078296</td>\n",
       "      <td>1.215762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.200000e+03</td>\n",
       "      <td>50.122056</td>\n",
       "      <td>-5.542976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>8.200680e+05</td>\n",
       "      <td>8.211545e+05</td>\n",
       "      <td>51.460578</td>\n",
       "      <td>-2.139327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.433289e+06</td>\n",
       "      <td>1.436002e+06</td>\n",
       "      <td>52.238142</td>\n",
       "      <td>-1.249739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2.105796e+06</td>\n",
       "      <td>2.107494e+06</td>\n",
       "      <td>53.370084</td>\n",
       "      <td>-0.171286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>2.643416e+06</td>\n",
       "      <td>2.643813e+06</td>\n",
       "      <td>55.184310</td>\n",
       "      <td>1.754630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_index_first_line  doc_index_last_line  onspdLatitude  \\\n",
       "count          9.270000e+02         9.270000e+02     927.000000   \n",
       "mean           1.431062e+06         1.433914e+06      52.338929   \n",
       "std            7.536094e+05         7.531904e+05       1.078296   \n",
       "min            0.000000e+00         1.200000e+03      50.122056   \n",
       "25%            8.200680e+05         8.211545e+05      51.460578   \n",
       "50%            1.433289e+06         1.436002e+06      52.238142   \n",
       "75%            2.105796e+06         2.107494e+06      53.370084   \n",
       "max            2.643416e+06         2.643813e+06      55.184310   \n",
       "\n",
       "       onspdLongitude  \n",
       "count      927.000000  \n",
       "mean        -1.213135  \n",
       "std          1.215762  \n",
       "min         -5.542976  \n",
       "25%         -2.139327  \n",
       "50%         -1.249739  \n",
       "75%         -0.171286  \n",
       "max          1.754630  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset by CQC classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Good                    598\n",
       "Requires improvement    218\n",
       "Outstanding              85\n",
       "Inadequate               26\n",
       "Name: rating_overall, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating_overall'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental health flag\n",
    "Creating mental health flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mental_flag'] = np.where(df['Location_subtype'].str.contains ('Mental'), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    630\n",
       "1    297\n",
       "Name: Mental_flag, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Mental_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a word count per sentence column\n",
    "\n",
    "def word_count(text):\n",
    "    # Find the total number of words in the tweet\n",
    "    total_words = len(word_tokenize(text))\n",
    "    \n",
    "    # Return the word counts\n",
    "    return total_words\n",
    "\n",
    "def sentence_count(text):\n",
    "    # Find the total number of words in the tweet\n",
    "    total_words = len(sent_tokenize(text))\n",
    "    \n",
    "    # Return the word counts\n",
    "    return total_words\n",
    "\n",
    "def distinct_word_count(text):\n",
    "    \n",
    "    # Find the total number of DISTINCT words in the tweet\n",
    "    total_distinct_words = len(set(w for w in word_tokenize(text)))\n",
    "    \n",
    "    return total_distinct_words\n",
    "\n",
    "def avg_word_len(text):\n",
    "    \n",
    "    avg_word_len = np.mean([len(w) for w in word_tokenize(text) ])\n",
    "    if np.isnan(avg_word_len):\n",
    "        avg_word_len = 0\n",
    "    else:\n",
    "        avg_word_len = avg_word_len\n",
    "    \n",
    "    return avg_word_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['full_text'].apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence_count'] = df['full_text'].apply(sentence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distinct_word_count'] = df['full_text'].apply(distinct_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_word_len'] = df['full_text'].apply(avg_word_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String transformation \n",
    "1. Amending stopwords list to INCLUDE word 'not'\n",
    "2. Limit dataset (remove intial document x lines that are related to the genral information about repors and CQC)\n",
    "3. Applying text transfomations to full text and on limited texts\n",
    "4. Additional transformations (creating lists, bigrams, pre-processing by class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amending the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))\n",
    "new_stopwords = set(stopwords.words('english')) \n",
    "new_stopwords.update('a',\n",
    "           'about',\n",
    "           'above',\n",
    "           'across',\n",
    "           'after',\n",
    "           'afterwards',\n",
    "           'again',\n",
    "           'against',\n",
    "           'all',\n",
    "           'almost',\n",
    "           'alone',\n",
    "           'along',\n",
    "           'already',\n",
    "           'also',\n",
    "           'although',\n",
    "           'always',\n",
    "           'am',\n",
    "           'among',\n",
    "           'amongst',\n",
    "           'amoungst',\n",
    "           'amount',\n",
    "           'an',\n",
    "           'and',\n",
    "           'another',\n",
    "           'any',\n",
    "           'anyhow',\n",
    "           'anyone',\n",
    "           'anything',\n",
    "           'anyway',\n",
    "           'anywhere',\n",
    "           'are',\n",
    "           'around',\n",
    "           'as',\n",
    "           'at',\n",
    "           'back',\n",
    "           'be',\n",
    "           'became',\n",
    "           'because',\n",
    "           'become',\n",
    "           'becomes',\n",
    "           'becoming',\n",
    "           'been',\n",
    "           'before',\n",
    "           'beforehand',\n",
    "           'behind',\n",
    "           'being',\n",
    "           'below',\n",
    "           'beside',\n",
    "           'besides',\n",
    "           'between',\n",
    "           'beyond',\n",
    "           'bill',\n",
    "           'both',\n",
    "           'bottom',\n",
    "           'but',\n",
    "           'by',\n",
    "           'call',\n",
    "           'can',\n",
    "           'cannot',\n",
    "           'cant',\n",
    "           'co',\n",
    "           'computer',\n",
    "           'con',\n",
    "           'could',\n",
    "           'couldnt',\n",
    "           'cry',\n",
    "           'de',\n",
    "           'describe',\n",
    "           'detail',\n",
    "           'did',\n",
    "           'didn',\n",
    "           'do',\n",
    "           'does',\n",
    "           'doesn',\n",
    "           'doing',\n",
    "           'don',\n",
    "           'done',\n",
    "           'down',\n",
    "           'due',\n",
    "           'during',\n",
    "           'each',\n",
    "           'eg',\n",
    "           'eight',\n",
    "           'either',\n",
    "           'eleven',\n",
    "           'else',\n",
    "           'elsewhere',\n",
    "           'empty',\n",
    "           'enough',\n",
    "           'etc',\n",
    "           'even',\n",
    "           'ever',\n",
    "           'every',\n",
    "           'everyone',\n",
    "           'everything',\n",
    "           'everywhere',\n",
    "           'except',\n",
    "           'few',\n",
    "           'fifteen',\n",
    "           'fifty',\n",
    "           'fill',\n",
    "           'find',\n",
    "           'fire',\n",
    "           'first',\n",
    "           'five',\n",
    "           'for',\n",
    "           'former',\n",
    "           'formerly',\n",
    "           'forty',\n",
    "           'found',\n",
    "           'four',\n",
    "           'from',\n",
    "           'front',\n",
    "           'full',\n",
    "           'further',\n",
    "           'get',\n",
    "           'give',\n",
    "           'go',\n",
    "           'had',\n",
    "           'has',\n",
    "           'hasnt',\n",
    "           'have',\n",
    "           'he',\n",
    "           'hence',\n",
    "           'her',\n",
    "           'here',\n",
    "           'hereafter',\n",
    "           'hereby',\n",
    "           'herein',\n",
    "           'hereupon',\n",
    "           'hers',\n",
    "           'herself',\n",
    "           'him',\n",
    "           'himself',\n",
    "           'his',\n",
    "           'how',\n",
    "           'however',\n",
    "           'hundred',\n",
    "           'i',\n",
    "           'ie',\n",
    "           'if',\n",
    "           'in',\n",
    "           'inc',\n",
    "           'indeed',\n",
    "           'interest',\n",
    "           'into',\n",
    "           'is',\n",
    "           'it',\n",
    "           'its',\n",
    "           'itself',\n",
    "           'just',\n",
    "           'keep',\n",
    "           'kg',\n",
    "           'km',\n",
    "           'last',\n",
    "           'latter',\n",
    "           'latterly',\n",
    "           'least',\n",
    "           'less',\n",
    "           'ltd',\n",
    "           'made',\n",
    "           'make',\n",
    "           'many',\n",
    "           'may',\n",
    "           'me',\n",
    "           'meanwhile',\n",
    "           'might',\n",
    "           'mill',\n",
    "           'mine',\n",
    "           'more',\n",
    "           'moreover',\n",
    "           'most',\n",
    "           'mostly',\n",
    "           'move',\n",
    "           'much',\n",
    "           'must',\n",
    "           'my',\n",
    "           'myself',\n",
    "           'name',\n",
    "           'namely',\n",
    "           'neither',\n",
    "           'never',\n",
    "           'nevertheless',\n",
    "           'next',\n",
    "           'nine',\n",
    "           'no',\n",
    "           'nobody',\n",
    "           'none',\n",
    "           'noone',\n",
    "           'nor',\n",
    "           'not',\n",
    "           'nothing',\n",
    "           'now',\n",
    "           'nowhere',\n",
    "           'of',\n",
    "           'off',\n",
    "           'often',\n",
    "           'on',\n",
    "           'once',\n",
    "           'one',\n",
    "           'only',\n",
    "           'onto',\n",
    "           'or',\n",
    "           'other',\n",
    "           'others',\n",
    "           'otherwise',\n",
    "           'our',\n",
    "           'ours',\n",
    "           'ourselves',\n",
    "           'out',\n",
    "           'over',\n",
    "           'own',\n",
    "           'part',\n",
    "           'per',\n",
    "           'perhaps',\n",
    "           'please',\n",
    "           'put',\n",
    "           'quite',\n",
    "           'rather',\n",
    "           're',\n",
    "           'really',\n",
    "           'regarding',\n",
    "           'same',\n",
    "           'say',\n",
    "           'see',\n",
    "           'seem',\n",
    "           'seemed',\n",
    "           'seeming',\n",
    "           'seems',\n",
    "           'serious',\n",
    "           'several',\n",
    "           'she',\n",
    "           'should',\n",
    "           'show',\n",
    "           'side',\n",
    "           'since',\n",
    "           'sincere',\n",
    "           'six',\n",
    "           'sixty',\n",
    "           'so',\n",
    "           'some',\n",
    "           'somehow',\n",
    "           'someone',\n",
    "           'something',\n",
    "           'sometime',\n",
    "           'sometimes',\n",
    "           'somewhere',\n",
    "           'still',\n",
    "           'such',\n",
    "           'system',\n",
    "           'take',\n",
    "           'ten',\n",
    "           'than',\n",
    "           'that',\n",
    "           'the',\n",
    "           'their',\n",
    "           'them',\n",
    "           'themselves',\n",
    "           'then',\n",
    "           'thence',\n",
    "           'there',\n",
    "           'thereafter',\n",
    "           'thereby',\n",
    "           'therefore',\n",
    "           'therein',\n",
    "           'thereupon',\n",
    "           'these',\n",
    "           'they',\n",
    "           'thick',\n",
    "           'thin',\n",
    "           'third',\n",
    "           'this',\n",
    "           'those',\n",
    "           'though',\n",
    "           'three',\n",
    "           'through',\n",
    "           'throughout',\n",
    "           'thru',\n",
    "           'thus',\n",
    "           'to',\n",
    "           'together',\n",
    "           'too',\n",
    "           'top',\n",
    "           'toward',\n",
    "           'towards',\n",
    "           'twelve',\n",
    "           'twenty',\n",
    "           'two',\n",
    "           'un',\n",
    "           'under',\n",
    "           'unless',\n",
    "           'until',\n",
    "           'up',\n",
    "           'upon',\n",
    "           'us',\n",
    "           'used',\n",
    "           'using',\n",
    "           'various',\n",
    "           'very',\n",
    "           'via',\n",
    "           'was',\n",
    "           'we',\n",
    "           'well',\n",
    "           'were',\n",
    "           'what',\n",
    "           'whatever',\n",
    "           'when',\n",
    "           'whence',\n",
    "           'whenever',\n",
    "           'where',\n",
    "           'whereafter',\n",
    "           'whereas',\n",
    "           'whereby',\n",
    "           'wherein',\n",
    "           'whereupon',\n",
    "           'wherever',\n",
    "           'whether',\n",
    "           'which',\n",
    "           'while',\n",
    "           'whither',\n",
    "           'who',\n",
    "           'whoever',\n",
    "           'whole',\n",
    "           'whom',\n",
    "           'whose',\n",
    "           'why',\n",
    "           'will',\n",
    "           'with',\n",
    "           'within',\n",
    "           'without',\n",
    "           'would',\n",
    "           'yet',\n",
    "           'you',\n",
    "           'your',\n",
    "           'yours',\n",
    "           'yourself',\n",
    "           'yourselves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = new_stopwords - {'not'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n",
      "better : worked\n"
     ]
    }
   ],
   "source": [
    "# check how WordNetLemmatizer is working  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\")) \n",
    "  \n",
    "# a denotes adjective in \"pos\" \n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))\n",
    "print(\"better :\", lemmatizer.lemmatize(\"worked\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_baseline_models(text):\n",
    "    tagged_sentence = nltk.tag.pos_tag(text.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    text = (' '.join(edited_sentence))\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    text = text.lstrip(' ')  \n",
    "    text = text.lower()\n",
    "    text = text.replace('&', ' and ')\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    text = text.replace('@', ' at ')\n",
    "\n",
    "    tokenized_words = word_tokenize(text.lower())\n",
    "    tokenized_words = [nltk.stem.WordNetLemmatizer().lemmatize(w) for w in tokenized_words]\n",
    "    tokenized_words = [word for word in tokenized_words if word not in new_stopwords]\n",
    "    tokenized_words = [word for word in tokenized_words if len(word)>2]\n",
    "    tokenized_words = [word for word in tokenized_words if not word.isnumeric()]\n",
    "    tokenized_words = \" \".join(tokenized_words)   \n",
    "    text = tokenized_words.replace(\"n't\", 'not')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I've got a bit of a working machine, & 2016 I.I must ensure, and not always look down. don't know what to do with it. Nuffield Hospital thinks it was not very good to keep it the original way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'got bit working machine must ensure not always look not know think not good keep original way'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_baseline_models(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new column with pre-processed text\n",
    "df['full_text_preprocess'] = df['full_text'].apply(preprocess_baseline_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit text data (finding key words and phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [doc_index_first_line, doc_details, doc_index_last_line, full_text, filename, providerId, locationId, organisationType, type, name, region, postalCode, onspdLatitude, onspdLongitude, rating_overall, reportDate, rating_caring, rating_effective, rating_responsive, rating_safe, rating_wellled, URL, Location_type, Location_subtype, Report_URL, Mental_flag, word_count, sentence_count, distinct_word_count, avg_word_len, full_text_preprocess, start_5, start_6, start_7]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# functions to find the phrases in the text (beginning) and return the position\n",
    "\n",
    "def find_phrase_5 (column):\n",
    "    if column.find('Overall summary')<3500: \n",
    "        start = column.find('Overall summary')\n",
    "    else:\n",
    "        start = -1\n",
    "    return start\n",
    "\n",
    "def find_phrase_6 (column):\n",
    "    if column.find('Summary of findings')<5000: \n",
    "        start = column.find('Summary of findings')\n",
    "    else:\n",
    "        start = -1\n",
    "    return start\n",
    "\n",
    "def find_phrase_7 (column):\n",
    "    if column.find('Overall  Information')<3500: \n",
    "        start = column.find('Overall  Information')\n",
    "    else:\n",
    "        start = -1\n",
    "    return start\n",
    "\n",
    "df['start_5'] = df['full_text'].apply(find_phrase_5)\n",
    "df['start_6'] = df['full_text'].apply(find_phrase_6)\n",
    "df['start_7'] = df['full_text'].apply(find_phrase_7)\n",
    "\n",
    "# check for errors - if happens that text doesnt have any of the above phrases\n",
    "print(df.loc[(df.start_5 == -1) & (df.start_6 == -1) & (df.start_7 == -1) ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index_first_line</th>\n",
       "      <th>doc_details</th>\n",
       "      <th>doc_index_last_line</th>\n",
       "      <th>full_text</th>\n",
       "      <th>filename</th>\n",
       "      <th>providerId</th>\n",
       "      <th>locationId</th>\n",
       "      <th>organisationType</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>...</th>\n",
       "      <th>Location_subtype</th>\n",
       "      <th>Report_URL</th>\n",
       "      <th>Mental_flag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>distinct_word_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>full_text_preprocess</th>\n",
       "      <th>start</th>\n",
       "      <th>full_text_limited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>1200</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>AAAA1704</td>\n",
       "      <td>RXR</td>\n",
       "      <td>RXR78</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>Blackburn Birthing Centre</td>\n",
       "      <td>...</td>\n",
       "      <td>Acute hospital - NHS non-specialist</td>\n",
       "      <td>https://www.cqc.org.uk/sites/default/files/new...</td>\n",
       "      <td>0</td>\n",
       "      <td>7741</td>\n",
       "      <td>366</td>\n",
       "      <td>1398</td>\n",
       "      <td>4.831675</td>\n",
       "      <td>doc url= http //www.cqc.org.uk/sites/default/f...</td>\n",
       "      <td>817</td>\n",
       "      <td>Summary of findings Letter from the Chief Insp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1200</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>2270</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>AAAA1791</td>\n",
       "      <td>RXH</td>\n",
       "      <td>RXH35</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>Bexhill Hospital</td>\n",
       "      <td>...</td>\n",
       "      <td>Acute hospital - NHS non-specialist</td>\n",
       "      <td>https://www.cqc.org.uk/sites/default/files/new...</td>\n",
       "      <td>0</td>\n",
       "      <td>6792</td>\n",
       "      <td>269</td>\n",
       "      <td>1233</td>\n",
       "      <td>4.695671</td>\n",
       "      <td>doc url= http //www.cqc.org.uk/sites/default/f...</td>\n",
       "      <td>806</td>\n",
       "      <td>Summary of findings Letter from the Chief Insp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2270</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>4073</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>AAAA1812</td>\n",
       "      <td>RXQ</td>\n",
       "      <td>RXQ51</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>Amersham Hospital</td>\n",
       "      <td>...</td>\n",
       "      <td>Acute hospital - NHS non-specialist</td>\n",
       "      <td>https://www.cqc.org.uk/sites/default/files/new...</td>\n",
       "      <td>0</td>\n",
       "      <td>11024</td>\n",
       "      <td>507</td>\n",
       "      <td>1763</td>\n",
       "      <td>4.864478</td>\n",
       "      <td>doc url= http //www.cqc.org.uk/sites/default/f...</td>\n",
       "      <td>914</td>\n",
       "      <td>Summary of findings Letter from the Chief Insp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4073</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>11841</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>AAAA2909</td>\n",
       "      <td>R1F</td>\n",
       "      <td>R1FAV</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>St Mary's Hospital (Mental Health Management)</td>\n",
       "      <td>...</td>\n",
       "      <td>Mental health - community &amp; residential - NHS</td>\n",
       "      <td>https://www.cqc.org.uk/sites/default/files/new...</td>\n",
       "      <td>1</td>\n",
       "      <td>55730</td>\n",
       "      <td>2310</td>\n",
       "      <td>3445</td>\n",
       "      <td>4.649955</td>\n",
       "      <td>doc url= http //www.cqc.org.uk/sites/default/f...</td>\n",
       "      <td>1505</td>\n",
       "      <td>Overall summary  4  The five questions we ask ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11841</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>17390</td>\n",
       "      <td>&lt;doc url=\"https://www.cqc.org.uk/sites/default...</td>\n",
       "      <td>AAAA2910</td>\n",
       "      <td>R1F</td>\n",
       "      <td>R1FX5</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>Community Healthcare Services, St Mary's Hospital</td>\n",
       "      <td>...</td>\n",
       "      <td>Community health - NHS &amp; Independent</td>\n",
       "      <td>https://www.cqc.org.uk/sites/default/files/new...</td>\n",
       "      <td>0</td>\n",
       "      <td>41845</td>\n",
       "      <td>1740</td>\n",
       "      <td>3373</td>\n",
       "      <td>4.776604</td>\n",
       "      <td>doc url= http //www.cqc.org.uk/sites/default/f...</td>\n",
       "      <td>1207</td>\n",
       "      <td>Overall summary  4  The five questions we ask ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_index_first_line                                        doc_details  \\\n",
       "0                     0  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "1                  1200  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "2                  2270  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "3                  4073  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "4                 11841  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "\n",
       "   doc_index_last_line                                          full_text  \\\n",
       "0                 1200  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "1                 2270  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "2                 4073  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "3                11841  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "4                17390  <doc url=\"https://www.cqc.org.uk/sites/default...   \n",
       "\n",
       "   filename providerId locationId organisationType  \\\n",
       "0  AAAA1704        RXR      RXR78         Location   \n",
       "1  AAAA1791        RXH      RXH35         Location   \n",
       "2  AAAA1812        RXQ      RXQ51         Location   \n",
       "3  AAAA2909        R1F      R1FAV         Location   \n",
       "4  AAAA2910        R1F      R1FX5         Location   \n",
       "\n",
       "                          type  \\\n",
       "0  NHS Healthcare Organisation   \n",
       "1  NHS Healthcare Organisation   \n",
       "2  NHS Healthcare Organisation   \n",
       "3  NHS Healthcare Organisation   \n",
       "4  NHS Healthcare Organisation   \n",
       "\n",
       "                                                name  ...  \\\n",
       "0                          Blackburn Birthing Centre  ...   \n",
       "1                                   Bexhill Hospital  ...   \n",
       "2                                  Amersham Hospital  ...   \n",
       "3      St Mary's Hospital (Mental Health Management)  ...   \n",
       "4  Community Healthcare Services, St Mary's Hospital  ...   \n",
       "\n",
       "                                Location_subtype  \\\n",
       "0            Acute hospital - NHS non-specialist   \n",
       "1            Acute hospital - NHS non-specialist   \n",
       "2            Acute hospital - NHS non-specialist   \n",
       "3  Mental health - community & residential - NHS   \n",
       "4           Community health - NHS & Independent   \n",
       "\n",
       "                                          Report_URL  Mental_flag  word_count  \\\n",
       "0  https://www.cqc.org.uk/sites/default/files/new...            0        7741   \n",
       "1  https://www.cqc.org.uk/sites/default/files/new...            0        6792   \n",
       "2  https://www.cqc.org.uk/sites/default/files/new...            0       11024   \n",
       "3  https://www.cqc.org.uk/sites/default/files/new...            1       55730   \n",
       "4  https://www.cqc.org.uk/sites/default/files/new...            0       41845   \n",
       "\n",
       "  sentence_count distinct_word_count avg_word_len  \\\n",
       "0            366                1398     4.831675   \n",
       "1            269                1233     4.695671   \n",
       "2            507                1763     4.864478   \n",
       "3           2310                3445     4.649955   \n",
       "4           1740                3373     4.776604   \n",
       "\n",
       "                                full_text_preprocess start  \\\n",
       "0  doc url= http //www.cqc.org.uk/sites/default/f...   817   \n",
       "1  doc url= http //www.cqc.org.uk/sites/default/f...   806   \n",
       "2  doc url= http //www.cqc.org.uk/sites/default/f...   914   \n",
       "3  doc url= http //www.cqc.org.uk/sites/default/f...  1505   \n",
       "4  doc url= http //www.cqc.org.uk/sites/default/f...  1207   \n",
       "\n",
       "                                   full_text_limited  \n",
       "0  Summary of findings Letter from the Chief Insp...  \n",
       "1  Summary of findings Letter from the Chief Insp...  \n",
       "2  Summary of findings Letter from the Chief Insp...  \n",
       "3  Overall summary  4  The five questions we ask ...  \n",
       "4  Overall summary  4  The five questions we ask ...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a start column fro the new start position for a text\n",
    "\n",
    "start = []\n",
    "for i in range(len(df['start_5'])):\n",
    "    one_value = max(df.iloc[i, 31:33])\n",
    "    start.append(one_value)\n",
    "\n",
    "df['start'] = start\n",
    "\n",
    "# function to limit text from the start posiont\n",
    "\n",
    "def limit_text(df, row):\n",
    "#     df.iloc[2, 23]\n",
    "    start = df.iloc[row, 34]\n",
    "    column = df.iloc[row, 3]\n",
    "    return column[start:]\n",
    "\n",
    "text_list = []\n",
    "for row in range(len(df['start_5'])):\n",
    "    one_text = (limit_text(df, row))\n",
    "    text_list.append(one_text)\n",
    "len(text_list)\n",
    "\n",
    "df['full_text_limited'] = text_list\n",
    "\n",
    "# cleaning dataset\n",
    "\n",
    "df = df.drop(['start_5', 'start_6', 'start_7' ], axis = 1)\n",
    "\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My transformation on full text limit preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing limit text \n",
    "df['full_text_limited_preprocess'] = df['full_text_limited'].apply(preprocess_baseline_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP process transformation\n",
    "\n",
    "'gentler' transformation, suitable more for the deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nlpprocess_models(text):\n",
    "    tagged_sentence = nltk.tag.pos_tag(text.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'NNP' and tag != 'NNPS']\n",
    "    tokenized_words = [word for word in edited_sentence if not word.isnumeric()]\n",
    "    text = \" \".join(tokenized_words) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_text_nlpprocess'] = df['full_text'].apply(preprocess_nlpprocess_models)\n",
    "df['full_text_limited_nlpprocess'] = df['full_text_limited'].apply(preprocess_nlpprocess_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"I've got a bit of a working machine, 2016 I , Why the full-stop don't know what to do with it. Nuffield Hospital thinks it was not very good to keep it the original way.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_nlpprocess_models(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other text details, word 'not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_not(text):\n",
    "    count_not = len(re.findall('not', text))\n",
    "    return count_not\n",
    "\n",
    "df['word_count_nlpprocess'] = df['full_text_nlpprocess'].apply(word_count)\n",
    "\n",
    "df['count_not'] = df['full_text_nlpprocess'].apply(count_not)\n",
    "\n",
    "def prop_not(text):\n",
    "    prop_not = len(re.findall('not', text))/df['word_count_nlpprocess'].shape[0]\n",
    "    return prop_not\n",
    "\n",
    "df['prop_not'] = df['full_text_nlpprocess'].apply(prop_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('rating_overall')['prop_not'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('CQC_documents_df_revised_tableau_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uni and Bigrams for Tableau Vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramReturner (text):\n",
    "    text = text.lower()\n",
    "    bigramFeatureVector = []\n",
    "    for item in nltk.bigrams(text.split()):\n",
    "        bigramFeatureVector.append(' '.join(item))\n",
    "    return bigramFeatureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bigram_full_text_preprocess'] = df['full_text_preprocess'].apply(bigramReturner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Good_score  = df.loc[df['rating_overall']== 'Good']\n",
    "df_RI_score  = df.loc[df['rating_overall']== 'Requires improvement']\n",
    "df_Outstanding_score  = df.loc[df['rating_overall']== 'Outstanding']\n",
    "df_Inadequate_score  = df.loc[df['rating_overall']== 'Inadequate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fd_bigrams_clean_Good= nltk.FreqDist(np.concatenate(list(df_Good_score['bigram_full_text_preprocess'])))\n",
    "fd_bigrams_clean_RI= nltk.FreqDist(np.concatenate(list(df_RI_score['bigram_full_text_preprocess'])))\n",
    "fd_bigrams_clean_Outstanding= nltk.FreqDist(np.concatenate(list(df_Outstanding_score['bigram_full_text_preprocess'])))\n",
    "fd_bigrams_clean_Inadequate= nltk.FreqDist(np.concatenate(list(df_Inadequate_score['bigram_full_text_preprocess'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullStr_Good = df_Good_score['full_text'].str.cat(sep= ' ')\n",
    "fullStr_RI = df_RI_score['full_text'].str.cat(sep= ' ')\n",
    "fullStr_Outstanding = df_Outstanding_score['full_text'].str.cat(sep= ' ')\n",
    "fullStr_Inadequate = df_Inadequate_score['full_text'].str.cat(sep= ' ')\n",
    "\n",
    "fullStr_Good_clean = preprocess_baseline_models(fullStr_Good)\n",
    "fd_words_clean_Good = nltk.FreqDist([w.lower() for w in word_tokenize(fullStr_Good_clean)])\n",
    "# fd_words_clean_Good.plot(20)\n",
    "\n",
    "fullStr_RI_clean = preprocess_baseline_models(fullStr_RI)\n",
    "fd_words_clean_RI = nltk.FreqDist([w.lower() for w in word_tokenize(fullStr_RI_clean)])\n",
    "# fd_words_clean_RI.plot(20)\n",
    "\n",
    "fullStr_Outstanding_clean = preprocess_baseline_models(fullStr_Outstanding)\n",
    "fd_words_clean_Outstanding = nltk.FreqDist([w.lower() for w in word_tokenize(fullStr_Outstanding_clean)])\n",
    "# fd_words_clean_Outstanding.plot(20)\n",
    "\n",
    "fullStr_Inadequate_clean = preprocess_baseline_models(fullStr_Inadequate)\n",
    "fd_words_clean_Inadequate = nltk.FreqDist([w.lower() for w in word_tokenize(fullStr_Inadequate_clean)])\n",
    "# fd_words_clean_Inadequate.plot(20)\n",
    "\n",
    "\n",
    "list_of_words_Good = list(set(word_tokenize(fullStr_Good_clean.lower())))  \n",
    "print('Number of words in Good category:', len(list_of_words_Good))\n",
    "list_of_words_RI = list(set(word_tokenize(fullStr_RI_clean.lower())))  \n",
    "print('Number of words in Requires Improvement category:', len(list_of_words_RI))\n",
    "list_of_words_Outstanding = list(set(word_tokenize(fullStr_Outstanding_clean.lower())))  \n",
    "print('Number of words in Outstanding category:', len(list_of_words_Outstanding))\n",
    "list_of_words_Inadequate = list(set(word_tokenize(fullStr_Inadequate_clean.lower())))  \n",
    "print('Number of words in Inadequate category:', len(list_of_words_Inadequate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Good = pd.DataFrame([dict(fd_words_clean_Good)]).T.reset_index()\n",
    "df_Inadequate = pd.DataFrame([dict(fd_words_clean_Inadequate)]).T.reset_index()\n",
    "df_Outstanding = pd.DataFrame([dict(fd_words_clean_Outstanding)]).T.reset_index()\n",
    "df_RI = pd.DataFrame([dict(fd_words_clean_RI)]).T.reset_index()\n",
    "\n",
    "df_Good['Score'] = 'Good'\n",
    "df_Inadequate['Score'] = 'Inadequate'\n",
    "df_Outstanding['Score'] = 'Outstanding'\n",
    "df_RI['Score'] = 'RI'\n",
    "\n",
    "frames = [df_Good, df_Inadequate, df_Outstanding, df_RI]\n",
    "df_Word_Frequency_by_class_unigrams = pd.concat(frames).rename(columns = {0: 'Frequency', 'index':'Word'})\n",
    "df_Word_Frequency_by_class_unigrams['type'] = 'unigram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Good = pd.DataFrame([dict(fd_bigrams_clean_Good)]).T.reset_index()\n",
    "df_Inadequate = pd.DataFrame([dict(fd_bigrams_clean_Inadequate)]).T.reset_index()\n",
    "df_Outstanding = pd.DataFrame([dict(fd_bigrams_clean_Outstanding)]).T.reset_index()\n",
    "df_RI = pd.DataFrame([dict(fd_bigrams_clean_RI)]).T.reset_index()\n",
    "\n",
    "df_Good['Score'] = 'Good'\n",
    "df_Inadequate['Score'] = 'Inadequate'\n",
    "df_Outstanding['Score'] = 'Outstanding'\n",
    "df_RI['Score'] = 'RI'\n",
    "\n",
    "frames = [df_Good, df_Inadequate, df_Outstanding, df_RI]\n",
    "df_Word_Frequency_by_class_bigrams = pd.concat(frames).rename(columns = {0: 'Frequency', 'index':'Word'})\n",
    "df_Word_Frequency_by_class_bigrams['type'] = 'bigram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_Word_Frequency_by_class_unigrams, df_Word_Frequency_by_class_bigrams]\n",
    "df_Word_Frequency_by_class = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting information about word frequency\n",
    "# df_Word_Frequency_by_class.to_csv('Word_Freq_by_class_tableau_v1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
