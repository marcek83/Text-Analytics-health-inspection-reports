{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\MA069ja\\\\Text-Analytics-health-inspection-reports'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# score metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "df = pd.read_csv(\"CQC_documents_df_revised_tableau_v1.csv\", index_col = 0)\n",
    "\n",
    "df['rating_overall'] = np.where((df['rating_overall'] =='Good') | (df['rating_overall'] =='Outstanding'), 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset  (927, 54)\n",
      "No. of unique classes 2\n",
      "Number of Unique Tokens 32362\n",
      "Shape of Label Tensor: (927,)\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters:\n",
    "max_features = 300\n",
    "maxlen = 40000\n",
    "batch_size = 32\n",
    "embedding_dims = 200\n",
    "filters = 192\n",
    "kernel_size = 3\n",
    "hidden_dims = 256\n",
    "epochs = 20\n",
    "\n",
    "print('Shape of dataset ',df.shape)\n",
    "print('No. of unique classes',len(set(df['rating_overall'])))\n",
    "\n",
    "\n",
    "# cleaning up the labels (classification 0,1 for classes in df)\n",
    "macronum=sorted(set(df['rating_overall']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "df['rating_overall']=df['rating_overall'].apply(fun)\n",
    "labels =  np.array(df['rating_overall'])\n",
    "\n",
    "# cleaning up the string\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = []\n",
    "for idx in range(df.full_text_limited_nlpprocess.shape[0]):\n",
    "    text = BeautifulSoup(df.full_text_limited_nlpprocess[idx])\n",
    "    texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "# changing text to sequences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# creating word_index of all vocab, vocab size set to len + 1\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "print('Shape of Label Tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (927, 40000)\n"
     ]
    }
   ],
   "source": [
    "# padding sequences (pre-padding)\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of Data Tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading hiw data\n",
    "Loading Welsh dataset for predictions at the end of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Welsh = pd.read_csv('Welsh_documents_df_revised_tableau_v1.csv', index_col = 0)\n",
    "\n",
    "texts = []\n",
    "# labels = []\n",
    "\n",
    "for idx in range(df_Welsh.full_text_limited_nlpprocess.shape[0]):\n",
    "#     print(idx)\n",
    "    text = BeautifulSoup(df_Welsh.full_text_limited_nlpprocess[idx])\n",
    "    texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "sequences_Welsh = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "x_Welsh = pad_sequences(sequences_Welsh, maxlen=maxlen)\n",
    "x_Welsh = sequence.pad_sequences(x_Welsh, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_idx = pd.read_csv('df_train_indices.csv', index_col = 0)\n",
    "df_test_idx = pd.read_csv('df_test_indices.csv', index_col = 0)\n",
    "\n",
    "x_train = data[np.array(df_train_idx.iloc[:, 0].values)]\n",
    "y_train = labels[np.array(df_train_idx.iloc[:, 0].values)]\n",
    "x_val = data[np.array(df_test_idx.iloc[:, 0].values)]\n",
    "y_val = labels[np.array(df_test_idx.iloc[:, 0].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649 train sequences\n",
      "278 test sequences\n",
      "x_train shape: (649, 40000)\n",
      "x_test shape: (278, 40000)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_val), 'test sequences')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for tuning with tuner\n",
    "from kerastuner.tuners import RandomSearch\n",
    "# from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "\n",
    "# libraries for training\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR= f\"{int(time.time())}\"\n",
    "\n",
    "def build_model (hp):\n",
    "    inputs = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    x = inputs\n",
    "    x_encoder = Embedding(max_features,embedding_dims,input_length=maxlen,)(x)\n",
    "    x_encoder = Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x_encoder)\n",
    "    bigrams = Conv1D(filters = hp.Int('filters', 32, 256, step=32),\n",
    "                     kernel_size=2,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(x_encoder)\n",
    "    bigrams = GlobalMaxPooling1D()(bigrams)\n",
    "    trigrams = Conv1D(filters = hp.Int('filters', 32, 256, step=32),\n",
    "                     kernel_size=3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(x_encoder)\n",
    "    trigrams = GlobalMaxPooling1D()(trigrams)\n",
    "    fourgrams = Conv1D(filters = hp.Int('filters', 32, 256, step=32),\n",
    "                     kernel_size=4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(x_encoder)\n",
    "    fourgrams = GlobalMaxPooling1D()(fourgrams)\n",
    "    merged = tf.keras.layers.concatenate([bigrams, trigrams, fourgrams], axis = 1)\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 5)):\n",
    "        merged = Dense(units=hp.Int('units_' + str(i),\n",
    "                                                min_value=32,\n",
    "                                                max_value=512, \n",
    "                                                step=32), activation='relu')(merged)\n",
    "    merged = Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(merged)\n",
    "    merged = Dense(1)(merged)\n",
    "    outputs = Activation('sigmoid')(merged)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch (\n",
    "        build_model, \n",
    "        objective = \"val_accuracy\",\n",
    "        max_trials = 5,\n",
    "        executions_per_trial = 3,\n",
    "        directory = LOG_DIR)\n",
    "\n",
    "tuner.search(x = x_train,\n",
    "            y = y_train,\n",
    "            epochs = 50,\n",
    "            batch_size = 32,\n",
    "            validation_data = (x_val, y_val),\n",
    "            verbose = 2,\n",
    "            callbacks=[keras.callbacks.EarlyStopping(patience=2)]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model parameter\n",
    "# {'dropout': 0.4, 'filters': 192, 'n_layers': 3, 'units_0': 320, \n",
    "#  'learning_rate': 0.0004894779804921969, 'units_1': 32, 'units_2': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "# from keras.layers import Dense, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "# import pickle\n",
    "# from collections import defaultdict\n",
    "# import sys\n",
    "# import os\n",
    "# os.environ['KERAS_BACKEND']='theano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40000, 200)   60000       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40000, 200)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 39999, 192)   76992       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 39998, 192)   115392      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 39997, 192)   153792      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 192)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 192)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 192)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 576)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 320)          184640      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           10272       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           1056        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            33          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 602,177\n",
      "Trainable params: 602,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 649 samples, validate on 278 samples\n",
      "Epoch 1/10\n",
      " - 655s - loss: 0.6032 - accuracy: 0.7196 - val_loss: 0.5919 - val_accuracy: 0.7266\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(max_features,embedding_dims,input_length=maxlen,)(tweet_input)\n",
    "tweet_encoder = merged = Dropout(0.4)(tweet_encoder)\n",
    "bigram_branch = Conv1D(filters=filters, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=filters, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=filters, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(320, activation='relu')(merged)\n",
    "merged = Dense(32, activation='relu')(merged)\n",
    "merged = Dense(32, activation='relu')(merged)\n",
    "\n",
    "merged = Dropout(0.4)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "optimizer = optimizers.Adam(lr=0.0004894779804921969)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10, # 30\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 2,\n",
    "           callbacks=[callbacks.EarlyStopping(patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(hist.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves : CNN_ngram (maxlen=40000, features=300)',fontsize=16)\n",
    "fig1.savefig('cnn_ngram_40000_300_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=plt.figure()\n",
    "plt.plot(hist.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves : CNN_ngram (maxlen=40000, features=300)',fontsize=16)\n",
    "fig2.savefig('cnn_ngram_accuracy40000_300.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.asarray(model.predict(x_val))\n",
    "\n",
    "print(metrics.accuracy_score(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "print(classification_report(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "y_pred = np.asarray(model.predict(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_CNN_ngram_40000_300.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_CNN_ngram_40000_300.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross_validation\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# seed = 42\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "# cvscores = []\n",
    "# for train, test in kfold.split(x_train, y_train):\n",
    "#     tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "#     tweet_encoder = Embedding(max_features,embedding_dims,input_length=maxlen,)(tweet_input)\n",
    "#     tweet_encoder = merged = Dropout(0.30000000000000004)(tweet_encoder)\n",
    "#     bigram_branch = Conv1D(filters=filters, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "#     bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "#     trigram_branch = Conv1D(filters=filters, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "#     trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "#     fourgram_branch = Conv1D(filters=filters, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "#     fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "#     merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "#     merged = Dense(448, activation='relu')(merged)\n",
    "#     merged = Dense(32, activation='relu')(merged)\n",
    "#     merged = Dense(32, activation='relu')(merged)\n",
    "    \n",
    "#     merged = Dropout(0.30000000000000004)(merged)\n",
    "#     merged = Dense(1)(merged)\n",
    "#     output = Activation('sigmoid')(merged)\n",
    "#     model = Model(inputs=[tweet_input], outputs=[output])\n",
    "#     optimizer = optimizers.Adam(lr=0.00013978522409411077)\n",
    "#     model.compile(loss='binary_crossentropy',\n",
    "#                       optimizer=optimizer,\n",
    "#                       metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     hist = model.fit(x_train, y_train,\n",
    "#               batch_size=32,\n",
    "#               epochs=50, \n",
    "#               validation_data=(x_val, y_val),\n",
    "#               verbose = 2,\n",
    "#                callbacks=[callbacks.EarlyStopping(patience=2)])\n",
    "    \n",
    "#     scores = model.evaluate(x_train[test], y_train[test], verbose=0)\n",
    "#     print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#     cvscores.append(scores[1] * 100)\n",
    "\n",
    "# print(\"Training Accuracy (cross-validation): %.2f%% \" % (np.mean(cvscores)))\n",
    "# print(\"Training Accuracy (standard deviation): %.2f%% \" % (np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model_CNN_ngram_40000_300.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json) # loaded model is the one to use with method compile\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_CNN_ngram_40000_300.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_Welsh = pd.read_csv('Welsh_predictions.csv', index_col = 0)\n",
    "\n",
    "Welsh_predict = loaded_model.predict(x_Welsh)\n",
    "\n",
    "Welsh_predict = np.where(Welsh_predict>0.5,1,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
