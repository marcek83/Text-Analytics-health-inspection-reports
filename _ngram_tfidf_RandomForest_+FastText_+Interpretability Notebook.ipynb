{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is divided into 3 parts:\n",
    "    1. n-grams and tf-idf language model implementation\n",
    "    2. Random Forest model (baseline, tuning, best model evaluation)\n",
    "    3. FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# libraries for tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Random Forest libraries\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# score metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "# fasttext libaries \n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.test.utils import datapath\n",
    "fb_path = datapath(\"wiki-news-300d-1M-subword.bin/wiki-news-300d-1M-subword.bin\") \n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "from gensim.models.fasttext import load_facebook_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set up\n",
    "\n",
    "The project will use unigrams and bigrams from the heavily pre-processed data (lower case, removing stopwords, lemmanisation) and mildly pre-processed data (lower case, removing limited stopwords)\n",
    "\n",
    "The classification will try to model the solution for 2 classes:\n",
    "- positive ('Outstanding' and 'Good')\n",
    "- negative ('Requires improvement' and 'Inadequate')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('x_train.csv', index_col = 0)\n",
    "Y_train = pd.read_csv('y_train.csv', index_col = 0)\n",
    "X_test = pd.read_csv('x_val.csv', index_col = 0).sort_index()\n",
    "Y_test = pd.read_csv('y_val.csv', index_col = 0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_idx = pd.read_csv('df_test_indices.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram + tfidf\n",
    "detecting best n features from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text_limited_preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>finding part hospital community hospital provi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        full_text_limited_preprocess\n",
       "2  finding part hospital community hospital provi..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (2, 4)\n",
    "\n",
    "# Limit on the number of features. We use the top 300 features.\n",
    "TOP_K = 300\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams + 3-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens and list.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "            'max_features' : TOP_K\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "    scores = vectorizer.get_feature_names()\n",
    "    \n",
    "\n",
    "#     scores = pd.DataFrame(list(zip(vectorizer.get_feature_names())),columns=['ftr'])\n",
    "\n",
    "#  Alternatively, Select top 'k' of the vectorized features.\n",
    "#     selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "#     selector.fit(x_train, train_labels)\n",
    "#     scores = pd.DataFrame(list(zip(vectorizer.get_feature_names(), selector.scores_, selector.pvalues_)), \n",
    "#                                        columns=['ftr', 'score', 'pval'])\n",
    "#     x_train = selector.transform(x_train).astype('float32')\n",
    "#     x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, scores = ngram_vectorize(X_train['full_text_limited_preprocess'], Y_train['rating_overall'], X_test['full_text_limited_preprocess'])\n",
    "x_train = x_train.toarray()\n",
    "x_val = x_val.toarray()\n",
    "y_train = Y_train.iloc[:, 0].values\n",
    "y_test = Y_test.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting features\n",
    "\n",
    "# scores = pd.DataFrame(scores, columns = ['word'])\n",
    "# scores.to_csv('rawTFIDF_feat2-4_top200.csv')\n",
    "\n",
    "# scores.to_csv('selectKbestTFIDF_feat2-4_top200.csv')\n",
    "\n",
    "# scores.sort_values(by=['score'], ascending = False)[:300].to_csv('top_300_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 1.0\n",
      "Test score 0.8705035971223022\n"
     ]
    }
   ],
   "source": [
    "# RF first run\n",
    "model = RandomForestClassifier(n_estimators=1000, class_weight = 'balanced')\n",
    "model.fit(x_train, y_train)\n",
    "print('Train score', model.score(x_train, y_train))\n",
    "print('Test score', model.score(x_val, y_test))\n",
    "\n",
    "y_pred = model.predict(x_val)\n",
    "y_pred_proba = model.predict_proba(x_val)\n",
    "y_pred_proba_list = []\n",
    "for item in y_pred_proba:\n",
    "    y_pred_proba_list.append(item[-1])\n",
    "y_pred_proba = np.asarray(y_pred_proba_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8705035971223022\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92       202\n",
      "           1       0.87      0.62      0.72        76\n",
      "\n",
      "    accuracy                           0.87       278\n",
      "   macro avg       0.87      0.79      0.82       278\n",
      "weighted avg       0.87      0.87      0.86       278\n",
      "\n",
      "[[195   7]\n",
      " [ 29  47]]\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "print(metrics.accuracy_score(y_test, np.where(y_pred_proba>0.5, 1,0)))\n",
    "\n",
    "print(classification_report(y_test, np.where(y_pred_proba>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_test, np.where(y_pred_proba>0.5, 1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "SEED = 5\n",
    "base = RandomForestClassifier()\n",
    "base.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Use feature selection to reduce number of columns to reduce unnecessary info.\n",
    "sfm = SelectFromModel(base, threshold=0.0005, prefit=True)\n",
    "X_t_train = sfm.transform(x_train)\n",
    "X_t_test = sfm.transform(x_val)\n",
    "# print(X.columns[sfm.get_support()]) # Print features selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_model_1 = RandomForestClassifier(class_weight='balanced_subsample', random_state = 1)\n",
    "print(rf_model_1.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 17.7min finished\n",
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                              class_weight='balanced_subsample',\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=N...\n",
       "                                              oob_score=False, random_state=1,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [True, False],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [3, 10, 15, 20],\n",
       "                         'max_features': ['auto', 'log2', 'sqrt'],\n",
       "                         'min_samples_leaf': [2, 5, 10],\n",
       "                         'n_estimators': [100, 200, 500, 1000]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define a grid of hyperparameter 'params_'\n",
    "\n",
    "param_grid_model_1 = {'n_estimators': [100, 200, 500, 1000], \n",
    "                      'max_features': ['auto', 'log2', \"sqrt\"],\n",
    "                      \"bootstrap\"    : [True, False],  \n",
    "                      'max_depth': [3, 10, 15, 20], \n",
    "                      'criterion': ['gini', 'entropy'],\n",
    "                      'min_samples_leaf':[2, 5, 10] \n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate 'grid_'\n",
    "grid_model_1 = GridSearchCV(estimator=rf_model_1,\n",
    "                               param_grid=param_grid_model_1, \n",
    "                               cv=5,\n",
    "                               scoring='f1',\n",
    "                               verbose=1,\n",
    "                               n_jobs=-1 \n",
    "                              )\n",
    "\n",
    "# Fit 'grid_' to the training set\n",
    "grid_model_1.fit(X_t_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      " {'bootstrap': False, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 10, 'n_estimators': 200}\n",
      "Training Accuracy (cross validation): 0.9183 \n",
      "0.05853977681273533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       481\n",
      "           1       0.91      1.00      0.95       168\n",
      "\n",
      "    accuracy                           0.98       649\n",
      "   macro avg       0.96      0.98      0.97       649\n",
      "weighted avg       0.98      0.98      0.98       649\n",
      "\n",
      "[[465  16]\n",
      " [  0 168]]\n"
     ]
    }
   ],
   "source": [
    "# Extract best hyperparameters from 'grid_rf'\n",
    "best_hyperparams_model_1 = grid_model_1.best_params_\n",
    "\n",
    "print('Best hyperparameters:\\n', best_hyperparams_model_1)\n",
    "\n",
    "# Extract best model from 'grid_svc'\n",
    "best_model_1 = grid_model_1.best_estimator_\n",
    "\n",
    "# cross-validate the model on train data\n",
    "cv_results = cross_val_score(best_model_1, X_t_train, y_train, cv=10) # returns of array of cross-validation scores\n",
    "\n",
    "y_pred_train = best_model_1.predict(X_t_train)\n",
    "y_pred_train_probs = best_model_1.predict_proba(X_t_train)[:, 1]\n",
    "\n",
    "\n",
    "#print('Best hyperparameters:\\n',cv_results) # the array len == cv (the array represents R2)\n",
    "print(\"Training Accuracy (cross validation): {:.4f} \".format(cv_results.mean()))\n",
    "print(cv_results.std())\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print (classification_report(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "\n",
    "\n",
    "#print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = best_model_1.predict_proba(X_t_test)\n",
    "y_pred_proba_list = []\n",
    "for item in y_pred_proba:\n",
    "    y_pred_proba_list.append(item[-1])\n",
    "y_pred_proba = np.asarray(y_pred_proba_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89568345323741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       202\n",
      "           1       0.81      0.82      0.81        76\n",
      "\n",
      "    accuracy                           0.90       278\n",
      "   macro avg       0.87      0.87      0.87       278\n",
      "weighted avg       0.90      0.90      0.90       278\n",
      "\n",
      "[[187  15]\n",
      " [ 14  62]]\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "print(metrics.accuracy_score(y_test, np.where(y_pred_proba>0.5, 1,0)))\n",
    "\n",
    "print(classification_report(y_test, np.where(y_pred_proba>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_test, np.where(y_pred_proba>0.5, 1,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>organisationType</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>region</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>onspdLatitude</th>\n",
       "      <th>onspdLongitude</th>\n",
       "      <th>rating_overall</th>\n",
       "      <th>reportDate</th>\n",
       "      <th>...</th>\n",
       "      <th>RF_300_24</th>\n",
       "      <th>CNN_ngram_20000</th>\n",
       "      <th>CNN_unigram_20000</th>\n",
       "      <th>CNN_unigram_20000_300</th>\n",
       "      <th>CNN_ngram_40000_200</th>\n",
       "      <th>LR_300_24</th>\n",
       "      <th>CNN_ngram_20000_300</th>\n",
       "      <th>CNN_ngram_40000_300</th>\n",
       "      <th>CNN_ngram_40000_200_FT</th>\n",
       "      <th>CNN_ngram_20000_300_FT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>Amersham Hospital</td>\n",
       "      <td>South East</td>\n",
       "      <td>HP7 0JD</td>\n",
       "      <td>51.663005</td>\n",
       "      <td>-0.621408</td>\n",
       "      <td>Requires improvement</td>\n",
       "      <td>6/20/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>Community Healthcare Services, St Mary's Hospital</td>\n",
       "      <td>South East</td>\n",
       "      <td>PO30 5TG</td>\n",
       "      <td>50.710843</td>\n",
       "      <td>-1.301330</td>\n",
       "      <td>Requires improvement</td>\n",
       "      <td>9/9/2014</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Location</td>\n",
       "      <td>NHS Healthcare Organisation</td>\n",
       "      <td>Cossham Hospital</td>\n",
       "      <td>South West</td>\n",
       "      <td>BS15 1LF</td>\n",
       "      <td>51.468887</td>\n",
       "      <td>-2.516120</td>\n",
       "      <td>Good</td>\n",
       "      <td>2/11/2015</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index organisationType                         type  \\\n",
       "0      2         Location  NHS Healthcare Organisation   \n",
       "1      4         Location  NHS Healthcare Organisation   \n",
       "2      6         Location  NHS Healthcare Organisation   \n",
       "\n",
       "                                                name      region postalCode  \\\n",
       "0                                  Amersham Hospital  South East    HP7 0JD   \n",
       "1  Community Healthcare Services, St Mary's Hospital  South East   PO30 5TG   \n",
       "2                                   Cossham Hospital  South West   BS15 1LF   \n",
       "\n",
       "   onspdLatitude  onspdLongitude        rating_overall reportDate  ...  \\\n",
       "0      51.663005       -0.621408  Requires improvement  6/20/2014  ...   \n",
       "1      50.710843       -1.301330  Requires improvement   9/9/2014  ...   \n",
       "2      51.468887       -2.516120                  Good  2/11/2015  ...   \n",
       "\n",
       "  RF_300_24  CNN_ngram_20000  CNN_unigram_20000  CNN_unigram_20000_300  \\\n",
       "0         1                1                  1                      1   \n",
       "1         0                1                  1                      0   \n",
       "2         0                0                  0                      0   \n",
       "\n",
       "   CNN_ngram_40000_200  LR_300_24  CNN_ngram_20000_300  CNN_ngram_40000_300  \\\n",
       "0                    1          1                    1                    1   \n",
       "1                    1          0                    0                    1   \n",
       "2                    0          0                    0                    0   \n",
       "\n",
       "   CNN_ngram_40000_200_FT  CNN_ngram_20000_300_FT  \n",
       "0                       1                       0  \n",
       "1                       0                       0  \n",
       "2                       0                       0  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predictions = pd.read_csv('actual_and_predictions_tbl.csv', index_col = 0)\n",
    "df_predictions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_predictions = df_predictions.drop('CNN_vanilla_20000', axis = 1)\n",
    "\n",
    "# df_predictions['y_actual_rf'] = y_test\n",
    "\n",
    "# df_predictions[:2]\n",
    "\n",
    "# df_predictions['RF_300_24'] = np.where(y_pred_proba>0.5, 1,0)\n",
    "\n",
    "# df_predictions.to_csv('actual_and_predictions_tbl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Welsh Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Welsh = pd.read_csv('Welsh_documents_df_revised_tableau_v1.csv', index_col = 0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Welsh = df_Welsh.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['level_0', 'Type of establishment', 'Services provided',\n",
       "       'Address line 1', 'Address line 2', 'Postcode',\n",
       "       'Last inspection date', 'report_url', 'filename_report',\n",
       "       'doc_index_first_line', 'doc_details', 'doc_index_last_line',\n",
       "       'full_text', 'word_count', 'sentence_count', 'distinct_word_count',\n",
       "       'avg_word_len', 'full_text_preprocess', 'start',\n",
       "       'full_text_limited', 'full_text_limited_preprocess',\n",
       "       'full_text_limited_preprocess_list', 'tokens_300', 'tokens_500',\n",
       "       'tokens_1000', 'tokens_2000', 'tokens_5000',\n",
       "       'full_text_limited_nlpprocess',\n",
       "       'full_text_limited_nlpprocess_list', 'tokens_500_nlpprocess',\n",
       "       'tokens_1000_nlpprocess', 'tokens_2000_nlpprocess',\n",
       "       'tokens_5000_nlpprocess', 'tokens_10000_nlpprocess',\n",
       "       'tokens_full_text_limited_doc2vec', 'full_text_nlpprocess',\n",
       "       'word_count_nlpprocess', 'count_not', 'prop_not', 'prop_not_2',\n",
       "       'bigram_full_text_preprocess', 'RF_300_24', 'human_check',\n",
       "       'TU_Name', 'CNN_ngram_20000_200', 'CNN_unigram_20000_200',\n",
       "       'CNN_unigram_20000_300', 'CNN_ngram_40000_200', 'LR_300_24',\n",
       "       'CNN_ngram_20000_300', 'CNN_ngram_40000_300',\n",
       "       'CNN_ngram_40000_200_FT', 'Google_reviews',\n",
       "       'CNN_ngram_20000_300_FT'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Welsh.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1616: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# ngram + tfidf\n",
    "\n",
    "x_train, x_Welsh, scores = ngram_vectorize(X_train['full_text_limited_preprocess'], \n",
    "                                    Y_train['rating_overall'], df_Welsh['full_text_limited_preprocess'])\n",
    "x_Welsh = x_Welsh.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check features len\n",
    "len(x_Welsh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform features \n",
    "X_t_Welsh = sfm.transform(x_Welsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions with the rf best model\n",
    "y_pred_proba_Welsh = best_model_1.predict_proba(X_t_Welsh)\n",
    "y_pred_proba_list = []\n",
    "for item in y_pred_proba_Welsh:\n",
    "    y_pred_proba_list.append(item[-1])\n",
    "y_pred_proba_Welsh = np.asarray(y_pred_proba_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_pred_proba_Welsh>0.5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welsh_predictions = pd.DataFrame(np.where(y_pred_proba_Welsh>0.5,1,0), columns =['RF_300_24'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText word-embeddings\n",
    "Instead of n-grams we can use fasttext word-embeddings either pre-trained by fb or pre-trained from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading previously pre-trained vectors on CQC data\n",
    "fb_model_mine = FastText.load('fasttext_full_text_limited_nlpprocess_list.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create word-embeddings, average word-embedding \n",
    "# the model name needs to be either the loaded fb vectors or fb trained on trained dataset vectors\n",
    "\n",
    "def get_feature_vector_ft(data_column):\n",
    "    index2word_set = set(fb_model_mine.wv.vocab.keys())  # words known to model\n",
    "    featureVec = np.zeros(fb_model_mine.vector_size, dtype=\"float32\")\n",
    "    featureVec \n",
    "    # Initialize a counter for number of words in a review\n",
    "    nwords = 0\n",
    "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "    for word in data_column:\n",
    "        if word in index2word_set: \n",
    "            featureVec = np.add(featureVec, fb_model_mine[word])\n",
    "            nwords += 1.\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    #print(featureVec)\n",
    "    return featureVec\n",
    "#train_data[\"tweets_sent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(649,)\n",
      "(278,)\n"
     ]
    }
   ],
   "source": [
    "# transforming train and test datasets\n",
    "\n",
    "X_train[\"feature_vec_ft_soft_clean\"] =X_train[\"full_text_limited_preprocess\"].apply(get_feature_vector_ft)\n",
    "X_train_ft_soft_clean = np.array(list(map(np.array, X_train.feature_vec_ft_soft_clean)))\n",
    "\n",
    "X_test[\"feature_vec_ft_soft_clean\"] =X_test[\"full_text_limited_preprocess\"].apply(get_feature_vector_ft)\n",
    "X_test_ft_soft_clean = np.array(list(map(np.array, X_test.feature_vec_ft_soft_clean)))\n",
    "\n",
    "print(X_train[\"feature_vec_ft_soft_clean\"].shape)\n",
    "print(X_test[\"feature_vec_ft_soft_clean\"].shape)\n",
    "\n",
    "y_train = Y_train.iloc[:, 0].values\n",
    "y_test = Y_test.iloc[:, 0].values\n",
    "\n",
    "x_train = np.array(X_train['feature_vec_ft_soft_clean'].tolist())\n",
    "x_val = np.array(X_test['feature_vec_ft_soft_clean'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': True, 'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 'warn', 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n",
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   31.5s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed: 11.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2880 out of 2880 | elapsed: 22.7min finished\n",
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      " {'bootstrap': False, 'criterion': 'entropy', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 10, 'n_estimators': 200}\n",
      "Training Accuracy (cross validation): 0.9167 \n",
      "Training sd (cross validation:  0.0578 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       481\n",
      "           1       0.92      1.00      0.96       168\n",
      "\n",
      "    accuracy                           0.98       649\n",
      "   macro avg       0.96      0.99      0.97       649\n",
      "weighted avg       0.98      0.98      0.98       649\n",
      "\n",
      "[[467  14]\n",
      " [  0 168]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "SEED = 5\n",
    "base = RandomForestClassifier()\n",
    "base.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Use feature selection to reduce number of columns to reduce unnecessary info.\n",
    "sfm = SelectFromModel(base, threshold=0.0005, prefit=True)\n",
    "X_t_train = sfm.transform(x_train)\n",
    "X_t_test = sfm.transform(x_val)\n",
    "# print(X.columns[sfm.get_support()]) # Print features selected\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_model_1 = RandomForestClassifier(class_weight='balanced_subsample', random_state = 1)\n",
    "print(rf_model_1.get_params())\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Define a grid of hyperparameter 'params_'\n",
    "param_grid_model_1 = {'n_estimators': [100, 200, 500, 1000], \n",
    "                      'max_features': ['auto', 'log2', \"sqrt\"],\n",
    "                      \"bootstrap\"    : [True, False],  \n",
    "                      'max_depth': [3, 10, 15, 20], \n",
    "                      'criterion': ['gini', 'entropy'],\n",
    "                      'min_samples_leaf':[2, 5, 10] \n",
    "}\n",
    "\n",
    "\n",
    "# Instantiate 'grid_'\n",
    "grid_model_1 = GridSearchCV(estimator=rf_model_1,\n",
    "                               param_grid=param_grid_model_1, \n",
    "                               cv=5,\n",
    "                               scoring='f1',\n",
    "                               verbose=1,\n",
    "                               n_jobs=-1 \n",
    "                              )\n",
    "\n",
    "# Fit 'grid_' to the training set\n",
    "grid_model_1.fit(X_t_train, y_train)\n",
    "\n",
    "# Extract best hyperparameters from 'grid_rf'\n",
    "best_hyperparams_model_1 = grid_model_1.best_params_\n",
    "\n",
    "print('Best hyperparameters:\\n', best_hyperparams_model_1)\n",
    "\n",
    "# Extract best model from 'grid_svc'\n",
    "best_model_1 = grid_model_1.best_estimator_\n",
    "\n",
    "# cross-validate the model on train data\n",
    "cv_results = cross_val_score(best_model_1, X_t_train, y_train, cv=10) # returns of array of cross-validation scores\n",
    "\n",
    "y_pred_train = best_model_1.predict(X_t_train)\n",
    "y_pred_train_probs = best_model_1.predict_proba(X_t_train)[:, 1]\n",
    "\n",
    "\n",
    "#print('Best hyperparameters:\\n',cv_results) # the array len == cv (the array represents R2)\n",
    "print(\"Training Accuracy (cross validation): {:.4f} \".format(cv_results.mean()))\n",
    "print(\"Training sd (cross validation:  {:.4f} \".format(cv_results.std()))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print (classification_report(y_train, y_pred_train))\n",
    "print(confusion_matrix(y_train, y_pred_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8885 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       202\n",
      "           1       0.79      0.80      0.80        76\n",
      "\n",
      "    accuracy                           0.89       278\n",
      "   macro avg       0.86      0.86      0.86       278\n",
      "weighted avg       0.89      0.89      0.89       278\n",
      "\n",
      "[[186  16]\n",
      " [ 15  61]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = best_model_1.predict_proba(X_t_test)\n",
    "y_pred_proba_list = []\n",
    "for item in y_pred_proba:\n",
    "    y_pred_proba_list.append(item[-1])\n",
    "y_pred_proba = np.asarray(y_pred_proba_list)\n",
    "\n",
    "print(\"Test Accuracy: {:.4f} \".format(metrics.accuracy_score(y_test, np.where(y_pred_proba>0.5, 1,0))))\n",
    "\n",
    "print(classification_report(y_test, np.where(y_pred_proba>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_test, np.where(y_pred_proba>0.5, 1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files: \n",
    "# 1. fb file\n",
    "# x_train, x_test, y_train, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
