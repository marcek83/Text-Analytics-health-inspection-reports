{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# score metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "df = pd.read_csv(\"CQC_documents_df_revised_tableau_v1.csv\", index_col = 0)\n",
    "\n",
    "df['rating_overall'] = np.where((df['rating_overall'] =='Good') | (df['rating_overall'] =='Outstanding'), 0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset  (927, 54)\n",
      "No. of unique classes 2\n",
      "Number of Unique Tokens 32362\n",
      "Shape of Label Tensor: (927,)\n"
     ]
    }
   ],
   "source": [
    "# set hyperparameters:\n",
    "max_features = 300\n",
    "maxlen = 40000\n",
    "batch_size = 32\n",
    "embedding_dims = 200\n",
    "filters = 192\n",
    "kernel_size = 3\n",
    "hidden_dims = 256\n",
    "epochs = 20\n",
    "\n",
    "print('Shape of dataset ',df.shape)\n",
    "print('No. of unique classes',len(set(df['rating_overall'])))\n",
    "\n",
    "\n",
    "# cleaning up the labels (classification 0,1 for classes in df)\n",
    "macronum=sorted(set(df['rating_overall']))\n",
    "macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n",
    "\n",
    "def fun(i):\n",
    "    return macro_to_id[i]\n",
    "\n",
    "df['rating_overall']=df['rating_overall'].apply(fun)\n",
    "labels =  np.array(df['rating_overall'])\n",
    "\n",
    "# cleaning up the string\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "texts = []\n",
    "for idx in range(df.full_text_limited_nlpprocess.shape[0]):\n",
    "    text = BeautifulSoup(df.full_text_limited_nlpprocess[idx])\n",
    "    texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "# changing text to sequences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# creating word_index of all vocab, vocab size set to len + 1\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "print('Number of Unique Tokens',len(word_index))\n",
    "print('Shape of Label Tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data Tensor: (927, 40000)\n"
     ]
    }
   ],
   "source": [
    "# padding sequences (pre-padding)\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "print('Shape of Data Tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading hiw data\n",
    "Loading Welsh dataset for predictions at the end of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Welsh = pd.read_csv('Welsh_documents_df_revised_tableau_v1.csv', index_col = 0)\n",
    "\n",
    "texts = []\n",
    "# labels = []\n",
    "\n",
    "for idx in range(df_Welsh.full_text_limited_nlpprocess.shape[0]):\n",
    "#     print(idx)\n",
    "    text = BeautifulSoup(df_Welsh.full_text_limited_nlpprocess[idx])\n",
    "    texts.append(clean_str(str(text.get_text().encode())))\n",
    "\n",
    "sequences_Welsh = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "x_Welsh = pad_sequences(sequences_Welsh, maxlen=maxlen)\n",
    "x_Welsh = sequence.pad_sequences(x_Welsh, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_idx = pd.read_csv('df_train_indices.csv', index_col = 0)\n",
    "df_test_idx = pd.read_csv('df_test_indices.csv', index_col = 0)\n",
    "\n",
    "x_train = data[np.array(df_train_idx.iloc[:, 0].values)]\n",
    "y_train = labels[np.array(df_train_idx.iloc[:, 0].values)]\n",
    "x_val = data[np.array(df_test_idx.iloc[:, 0].values)]\n",
    "y_val = labels[np.array(df_test_idx.iloc[:, 0].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649 train sequences\n",
      "278 test sequences\n",
      "x_train shape: (649, 40000)\n",
      "x_test shape: (278, 40000)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_val), 'test sequences')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for tuning with tuner\n",
    "from kerastuner.tuners import RandomSearch\n",
    "# from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import time\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "\n",
    "# libraries for training\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalMaxPooling1D\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import kerastuner as kt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR= f\"{int(time.time())}\"\n",
    "\n",
    "def build_model (hp):\n",
    "    inputs = tf.keras.Input(shape=(maxlen,), dtype='int32')\n",
    "    x = inputs\n",
    "    x_encoder = Embedding(max_features,embedding_dims,input_length=maxlen,)(x)\n",
    "    x_encoder = Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(x_encoder)\n",
    "    bigrams = Conv1D(filters = hp.Int('filters', 32, 256, step=32),\n",
    "                     kernel_size=2,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(x_encoder)\n",
    "    bigrams = GlobalMaxPooling1D()(bigrams)\n",
    "    trigrams = Conv1D(filters = hp.Int('filters', 32, 256, step=32),\n",
    "                     kernel_size=3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(x_encoder)\n",
    "    trigrams = GlobalMaxPooling1D()(trigrams)\n",
    "    fourgrams = Conv1D(filters = hp.Int('filters', 32, 256, step=32),\n",
    "                     kernel_size=4,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1)(x_encoder)\n",
    "    fourgrams = GlobalMaxPooling1D()(fourgrams)\n",
    "    merged = tf.keras.layers.concatenate([bigrams, trigrams, fourgrams], axis = 1)\n",
    "    for i in range(hp.Int(\"n_layers\", 1, 5)):\n",
    "        merged = Dense(units=hp.Int('units_' + str(i),\n",
    "                                                min_value=32,\n",
    "                                                max_value=512, \n",
    "                                                step=32), activation='relu')(merged)\n",
    "    merged = Dropout(hp.Float('dropout', 0, 0.5, step=0.1, default=0.5))(merged)\n",
    "    merged = Dense(1)(merged)\n",
    "    outputs = Activation('sigmoid')(merged)\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch (\n",
    "        build_model, \n",
    "        objective = \"val_accuracy\",\n",
    "        max_trials = 5,\n",
    "        executions_per_trial = 3,\n",
    "        directory = LOG_DIR)\n",
    "\n",
    "tuner.search(x = x_train,\n",
    "            y = y_train,\n",
    "            epochs = 50,\n",
    "            batch_size = 32,\n",
    "            validation_data = (x_val, y_val),\n",
    "            verbose = 2,\n",
    "            callbacks=[keras.callbacks.EarlyStopping(patience=2)]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model parameter\n",
    "# {'dropout': 0.4, 'filters': 192, 'n_layers': 3, 'units_0': 320, \n",
    "#  'learning_rate': 0.0004894779804921969, 'units_1': 32, 'units_2': 32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import callbacks\n",
    "\n",
    "\n",
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "# from keras.layers import Dense, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, LSTM, GRU, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "# from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "# import pickle\n",
    "# from collections import defaultdict\n",
    "# import sys\n",
    "# import os\n",
    "# os.environ['KERAS_BACKEND']='theano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40000)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40000, 200)   60000       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40000, 200)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 39999, 192)   76992       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 39998, 192)   115392      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 39997, 192)   153792      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 192)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 192)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 192)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 576)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 320)          184640      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 32)           10272       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           1056        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            33          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 602,177\n",
      "Trainable params: 602,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA069ja\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 649 samples, validate on 278 samples\n",
      "Epoch 1/10\n",
      " - 655s - loss: 0.6032 - accuracy: 0.7196 - val_loss: 0.5919 - val_accuracy: 0.7266\n",
      "Epoch 2/10\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(max_features,embedding_dims,input_length=maxlen,)(tweet_input)\n",
    "tweet_encoder = merged = Dropout(0.4)(tweet_encoder)\n",
    "bigram_branch = Conv1D(filters=filters, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=filters, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=filters, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(320, activation='relu')(merged)\n",
    "merged = Dense(32, activation='relu')(merged)\n",
    "merged = Dense(32, activation='relu')(merged)\n",
    "\n",
    "merged = Dropout(0.4)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "optimizer = optimizers.Adam(lr=0.0004894779804921969)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10, # 30\n",
    "          validation_data=(x_val, y_val),\n",
    "          verbose = 2,\n",
    "           callbacks=[callbacks.EarlyStopping(patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(hist.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves : CNN_ngram (maxlen=40000, features=300)',fontsize=16)\n",
    "fig1.savefig('cnn_ngram_40000_300_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=plt.figure()\n",
    "plt.plot(hist.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves : CNN_ngram (maxlen=40000, features=300)',fontsize=16)\n",
    "fig2.savefig('cnn_ngram_accuracy40000_300.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.asarray(model.predict(x_val))\n",
    "\n",
    "print(metrics.accuracy_score(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "print(classification_report(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "y_pred = np.asarray(model.predict(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model_CNN_ngram_40000_300.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_CNN_ngram_40000_300.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "seed = 42\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(x_train, y_train):\n",
    "    tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "    tweet_encoder = Embedding(max_features,embedding_dims,input_length=maxlen,)(tweet_input)\n",
    "    tweet_encoder = merged = Dropout(0.30000000000000004)(tweet_encoder)\n",
    "    bigram_branch = Conv1D(filters=filters, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "    bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "    trigram_branch = Conv1D(filters=filters, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "    trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "    fourgram_branch = Conv1D(filters=filters, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "    fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "    merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "    merged = Dense(448, activation='relu')(merged)\n",
    "    merged = Dense(32, activation='relu')(merged)\n",
    "    merged = Dense(32, activation='relu')(merged)\n",
    "    \n",
    "    merged = Dropout(0.30000000000000004)(merged)\n",
    "    merged = Dense(1)(merged)\n",
    "    output = Activation('sigmoid')(merged)\n",
    "    model = Model(inputs=[tweet_input], outputs=[output])\n",
    "    optimizer = optimizers.Adam(lr=0.00013978522409411077)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=50, \n",
    "              validation_data=(x_val, y_val),\n",
    "              verbose = 2,\n",
    "               callbacks=[callbacks.EarlyStopping(patience=2)])\n",
    "    \n",
    "    scores = model.evaluate(x_train[test], y_train[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "print(\"Training Accuracy (cross-validation): %.2f%% \" % (np.mean(cvscores)))\n",
    "print(\"Training Accuracy (standard deviation): %.2f%% \" % (np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('model_CNN_ngram_40000_300.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json) # loaded model is the one to use with method compile\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_CNN_ngram_40000_300.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_Welsh = pd.read_csv('Welsh_predictions.csv', index_col = 0)\n",
    "\n",
    "Welsh_predict = loaded_model.predict(x_Welsh)\n",
    "\n",
    "Welsh_predict = np.where(Welsh_predict>0.5,1,0)\n",
    "\n",
    "predictions_Welsh['CNN_ngram_40000_300'] = Welsh_predict\n",
    "\n",
    "predictions_Welsh.to_csv('Welsh_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Welsh = pd.read_csv('Welsh_documents_df_revised_tableau_v1.csv', index_col = 0)\n",
    "\n",
    "df_Welsh['CNN_ngram_40000_300'] = np.where(Welsh_predict>0.5,1,0)\n",
    "\n",
    "df_Welsh.to_csv('Welsh_documents_df_revised_tableau_v1.csv')\n",
    "df_Welsh[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_explanation_words(encoded_x):\n",
    "    words = word_index\n",
    "    num2word = {}\n",
    "    for w in words.keys():\n",
    "        num2word[words[w]] = w\n",
    "    x_test_words = np.stack([\n",
    "        np.array(list(map(\n",
    "            lambda x: num2word.get(x, \"NONE\"), encoded_x[i])\n",
    "                     )) for i in range(10)])\n",
    "\n",
    "    return x_test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_explanation_words(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# kernel_explainer = shap.KernelExplainer(pipeline.model.predict, encoded_x_train[:10])\n",
    "# kernel_shap_values = kernel_explainer.shap_values(encoded_x_test[:1])\n",
    "\n",
    "# x_test_words = prepare_explanation_words(pipeline, encoded_x_test)\n",
    "# y_pred = pipeline.predict(x_test[:1])\n",
    "# print('Actual Category: %s, Predict Category: %s' % (y_test[0], y_pred[0]))\n",
    "\n",
    "# shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][0], x_test_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kernel_explainer = shap.KernelExplainer(model.predict, x_train[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_values = kernel_explainer.shap_values(x_val[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_words = prepare_explanation_words(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "y_pred = model.predict_classes(x_val[:1])\n",
    "print('Actual Category: %s, Predict Category: %s' % (y_val[0], y_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][0], x_test_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explainer = shap.GradientExplainer(model.predict, x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import shap\n",
    "# explainer = shap.KernelExplainer(model.predict, x_train[:10])\n",
    "# shap_values = explainer.shap_values(x_val[:10], nsamples=10)\n",
    "# # shap_values = explainer.shap_values(X.iloc[299,:], nsamples=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(kernel_shap_values, x_val[:10], feature_names=x_test_words[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value[0], shap_values[0], x_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hc_utils.shap_deep import TFDeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "print('Build model...')\n",
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(max_features,embedding_dims,input_length=maxlen,)(tweet_input)\n",
    "tweet_encoder = merged = Dropout(0.2)(tweet_encoder)\n",
    "bigram_branch = Conv1D(filters=filters, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=filters, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=filters, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(hist.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN',fontsize=16)\n",
    "fig1.savefig('loss_rnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=plt.figure()\n",
    "plt.plot(hist.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves : RNN',fontsize=16)\n",
    "fig2.savefig('accuracy_rnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.asarray(model.predict(x_val))\n",
    "# score metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "print(classification_report(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_val, np.where(y_pred>0.5, 1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 200\n",
    "maxlen = 40000\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "filters = 100\n",
    "kernel_size = 3\n",
    "hidden_dims = 256\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('C:/Users/MA069ja/cs224u/data/glove.6B/glove.6B.100d.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, embedding_dims))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(vocab_size,embedding_dims,input_length=maxlen,weights=[embedding_matrix],trainable = False)(tweet_input)\n",
    "tweet_encoder = merged = Dropout(0.2)(tweet_encoder)\n",
    "bigram_branch = Conv1D(filters=filters, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=filters, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=filters, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "plt.plot(hist.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN',fontsize=16)\n",
    "fig1.savefig('loss_rnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=plt.figure()\n",
    "plt.plot(hist.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves : RNN',fontsize=16)\n",
    "fig2.savefig('accuracy_rnn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.asarray(model.predict(x_val))\n",
    "# score metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "print(classification_report(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_val, np.where(y_pred>0.5, 1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, concatenate, Activation\n",
    "from keras.models import Model\n",
    "\n",
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(vocab_size,embedding_dims,input_length=maxlen,weights=[embedding_matrix],trainable = True)(tweet_input)\n",
    "tweet_encoder = merged = Dropout(0.2)(tweet_encoder)\n",
    "bigram_branch = Conv1D(filters=filters, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=filters, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=filters, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.plot(hist.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves :RNN',fontsize=16)\n",
    "fig1.savefig('loss_rnn.png')\n",
    "plt.show()\n",
    "\n",
    "fig2=plt.figure()\n",
    "plt.plot(hist.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(hist.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves : RNN',fontsize=16)\n",
    "fig2.savefig('accuracy_rnn.png')\n",
    "plt.show()\n",
    "\n",
    "y_pred = np.asarray(model.predict(x_val))\n",
    "# score metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "print(classification_report(y_val, np.where(y_pred>0.5, 1,0)))\n",
    "\n",
    "# Print confusion matrix using predictions\n",
    "print(confusion_matrix(y_val, np.where(y_pred>0.5, 1,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(len(word_index) + 1,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen,\n",
    "# #                    weights=[embedding_matrix],\n",
    "# #                    trainable = True\n",
    "#                    ))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# # we add a Convolution1D, which will learn filters\n",
    "# # word group filters of size filter_length:\n",
    "# model.add(Conv1D(filters,\n",
    "#                  kernel_size,\n",
    "#                  padding='valid',\n",
    "#                  activation='relu',\n",
    "#                  strides=1))\n",
    "# # we use max pooling:\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(LSTM(100))\n",
    "# # We add a vanilla hidden layer:\n",
    "# model.add(Dense(hidden_dims))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(1))\n",
    "# model.add(Activation('sigmoid'))\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# hist = model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 10\n",
    "#import spacy\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "\n",
    "def model_train_cv(x_train,y_train,nfold,model_obj):\n",
    "    splits = list(StratifiedKFold(n_splits=nfold, shuffle=True, random_state=SEED).split(x_train, y_train))\n",
    "    x_train = x_train\n",
    "    y_train = np.array(y_train)\n",
    "    # matrix for the out-of-fold predictions\n",
    "    train_oof_preds = np.zeros((x_train.shape[0]))\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f'Fold {i + 1}')\n",
    "        x_train_fold = x_train[train_idx.astype(int)]\n",
    "        y_train_fold = y_train[train_idx.astype(int)]\n",
    "        x_val_fold = x_train[valid_idx.astype(int)]\n",
    "        y_val_fold = y_train[valid_idx.astype(int)]\n",
    "\n",
    "        clf = copy.deepcopy(model_obj)\n",
    "        clf.fit(x_train_fold, y_train_fold, batch_size=512, epochs=5, validation_data=(x_val_fold, y_val_fold))\n",
    "        \n",
    "        valid_preds_fold = clf.predict(x_val_fold)[:,0]\n",
    "\n",
    "        # storing OOF predictions\n",
    "        train_oof_preds[valid_idx] = valid_preds_fold\n",
    "    return train_oof_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_cv(x_train, y_train, 5, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "convs = []\n",
    "filter_sizes = [2,3,4,5,6]\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=200, \n",
    "             kernel_size=filter_size, \n",
    "            activation='relu')(embedded_sequences)\n",
    "    l_pool = GlobalMaxPooling1D()(l_conv)\n",
    "    convs.append(l_pool)\n",
    "l_merge = concatenate(convs, axis=1)\n",
    "x = Dropout(0.1)(l_merge)  \n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "preds = Dense(2, activation='sigmoid')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
